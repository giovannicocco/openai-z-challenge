{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Tracing Nhamini-wi — In Search of the Lost Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "- [Introduction](#introduction-tracing-nhamini-wi--in-search-of-the-lost-path)\n",
    "- [Benchmark: Establishing a Baseline in the Unknown](#benchmark-establishing-a-baseline-in-the-unknown)\n",
    "- [Google Earth Engine Authentication](#google-earth-engine-authentication)\n",
    "- [From Data to Discovery](#from-data-to-discovery)\n",
    "- [Sensing the Unknown](#sensing-the-unknown)\n",
    "- [Comparing Legends and Landmarks](#comparing-legends-and-landmarks)\n",
    "- [AI Assessment: Insights from the Digital Expedition](#ai-assessment-insights-from-the-digital-expedition)\n",
    "- [Seeing the Unseen: Visualizing the Candidate Site](#seeing-the-unseen-visualizing-the-candidate-site)\n",
    "- [Leveraging Candidate Analysis for Future Discoveries](#leveraging-candidate-analysis-for-future-discoveries)\n",
    "- [Project Structure](#project-structure)\n",
    "- [Methodology](#methodology)\n",
    "- [Knowing Issues](#issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every legend begins with a mystery. Deep in the tangled forests of northwestern Amazonia, the trail of Nhamini-wi lingers—an ancient route that echoes through Indigenous memory and the journals of explorers who dared to cross these lands. Some say it was a road paved with stone, others whisper of geometric earthworks and signs of vanished civilizations.\n",
    "\n",
    "Guided by these tales, I set out not with a machete, but with code and curiosity. My expedition would not fight vines or wild rivers, but data: petabytes of open-source satellite imagery, remote sensing indices, and the pattern-finding powers of modern AI. Somewhere within these swirling pixels, I hoped, traces of the lost path might reveal themselves.\n",
    "\n",
    "I focused my digital search on the Brazilian Amazon near the headwaters of the Içana and Uaupés, ever watchful for clues crossing the invisible frontiers into Colombia and Venezuela. This journey isn’t only about data—it’s about making the invisible visible again, about uncovering forgotten stories written on the land itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: Establishing a Baseline in the Unknown\n",
    "\n",
    "Every explorer needs a compass. Before venturing into the unmapped, I needed to calibrate my digital senses—to distinguish the unusual from the expected. I turned to Acre, a region where decades of fieldwork have already uncovered monumental geoglyphs and ancient sites. These known locations would serve as my reference points, my “control group” in the jungle.\n",
    "\n",
    "Armed with AI, I asked for coordinates of these archaeological benchmarks. Their signatures in the data—NDVI, SRTM, radar, land cover—would help me recognize what a true anomaly might look like deeper in the unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T12:49:26.474901Z",
     "iopub.status.busy": "2025-06-23T12:49:26.474608Z",
     "iopub.status.idle": "2025-06-23T12:49:56.026456Z",
     "shell.execute_reply": "2025-06-23T12:49:56.025524Z",
     "shell.execute_reply.started": "2025-06-23T12:49:26.474880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fazenda Colorada Geoglyph</td>\n",
       "      <td>-10.562</td>\n",
       "      <td>-67.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fazenda Atlântica Geoglyph</td>\n",
       "      <td>-9.372</td>\n",
       "      <td>-67.873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jacó Sá (Santa Maria) Geoglyph</td>\n",
       "      <td>-9.668</td>\n",
       "      <td>-67.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fazenda Tequinho Geoglyph</td>\n",
       "      <td>-10.061</td>\n",
       "      <td>-67.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Antimary Forest Geoglyph</td>\n",
       "      <td>-9.816</td>\n",
       "      <td>-67.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sol de Mâncio Lima Geoglyph</td>\n",
       "      <td>-7.620</td>\n",
       "      <td>-72.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>São Luiz do Remanso Geoglyph</td>\n",
       "      <td>-9.944</td>\n",
       "      <td>-67.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bamburral (Paranaíba) Geoglyph</td>\n",
       "      <td>-10.571</td>\n",
       "      <td>-67.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tangará Geoglyph</td>\n",
       "      <td>-10.370</td>\n",
       "      <td>-67.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alto Alegre Geoglyph</td>\n",
       "      <td>-10.291</td>\n",
       "      <td>-67.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name     lat     lon\n",
       "0       Fazenda Colorada Geoglyph -10.562 -67.260\n",
       "1      Fazenda Atlântica Geoglyph  -9.372 -67.873\n",
       "2  Jacó Sá (Santa Maria) Geoglyph  -9.668 -67.668\n",
       "3       Fazenda Tequinho Geoglyph -10.061 -67.263\n",
       "4        Antimary Forest Geoglyph  -9.816 -67.276\n",
       "5     Sol de Mâncio Lima Geoglyph  -7.620 -72.920\n",
       "6    São Luiz do Remanso Geoglyph  -9.944 -67.893\n",
       "7  Bamburral (Paranaíba) Geoglyph -10.571 -67.208\n",
       "8                Tangará Geoglyph -10.370 -67.290\n",
       "9            Alto Alegre Geoglyph -10.291 -67.165"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] OpenAI model used: o3\n",
      "\n",
      "Prompt tokens: 215\n",
      "Completion tokens: 1418\n",
      "Total tokens: 1633\n"
     ]
    }
   ],
   "source": [
    "# benchmark.py\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "\n",
    "# Load OpenAI API key from Kaggle secrets or environment\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    openai_key = user_secrets.get_secret(\"openai\")\n",
    "except Exception:\n",
    "    import os\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=openai_key) if openai_key else OpenAI()\n",
    "\n",
    "# Define Pydantic models for structured output\n",
    "class BenchmarkSite(BaseModel):\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "\n",
    "class BenchmarkSites(BaseModel):\n",
    "    sites: list[BenchmarkSite]\n",
    "\n",
    "# Prompt for OpenAI Structured Output (expects a JSON object with a 'sites' key)\n",
    "prompt = (\n",
    "    \"You are an archaeologist specialized in the Amazon region.\\n\"\n",
    "    \"List at least 10 known archaeological sites located in the state of Acre, Brazil, \"\n",
    "    \"including their approximate latitude and longitude.\\n\"\n",
    "    \"Return ONLY a JSON object with a 'sites' key, which is a list of objects with fields: name (string), lat (number), lon (number).\\n\"\n",
    "    \"Example: {\\\"sites\\\": [{\\\"name\\\": \\\"Site Name\\\", \\\"lat\\\": -X.XXXX, \\\"lon\\\": -Y.YYYY}]}\\n\"\n",
    "    \"Focus on geoglyphs and earthworks documented in academic literature or official records.\\n\"\n",
    ")\n",
    "\n",
    "# Call OpenAI API and parse with Pydantic Structured Output\n",
    "response = client.responses.parse(\n",
    "    model=\"o3\",\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    text_format=BenchmarkSites,\n",
    ")\n",
    "\n",
    "sites = response.output_parsed.sites\n",
    "df_benchmark = pd.DataFrame([s.model_dump() for s in sites])\n",
    "\n",
    "# Adiciona coluna Google Maps antes do display\n",
    "def make_gmaps_link(lat, lon):\n",
    "    url = f'https://www.google.com/maps/search/?api=1&query={lat},{lon}'\n",
    "    return f'<a href=\"{url}\" target=\"_blank\">Ver no Google Maps</a>'\n",
    "\n",
    "df_benchmark['Google Maps'] = df_benchmark.apply(lambda row: make_gmaps_link(row['lat'], row['lon']), axis=1)\n",
    "\n",
    "# Display DataFrame in notebook/Kaggle environment, fallback to print\n",
    "try:\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(df_benchmark.to_html(escape=False)))\n",
    "except Exception:\n",
    "    print(df_benchmark)\n",
    "\n",
    "# Print model version used\n",
    "print(f\"\\n[INFO] OpenAI model used: o3\")\n",
    "\n",
    "# Print token usage if available\n",
    "usage = getattr(response, \"usage\", None)\n",
    "if usage:\n",
    "    print(f\"\\nPrompt tokens: {getattr(usage, 'prompt_tokens', getattr(usage, 'input_tokens', None))}\")\n",
    "    print(f\"Completion tokens: {getattr(usage, 'completion_tokens', getattr(usage, 'output_tokens', None))}\")\n",
    "    print(f\"Total tokens: {getattr(usage, 'total_tokens', None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir DataFrame de benchmarks com links para Google Maps\n",
    "from IPython.display import display, HTML\n",
    "def make_gmaps_link(lat, lon):\n",
    "    url = f'https://www.google.com/maps/search/?api=1&query={lat},{lon}'\n",
    "    return f'<a href=\"{url}\" target=\"_blank\">Ver no Google Maps</a>'\n",
    "\n",
    "df_benchmark['Google Maps'] = df_benchmark.apply(lambda row: make_gmaps_link(row['lat'], row['lon']), axis=1)\n",
    "display(HTML(df_benchmark.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Earth Engine Authentication\n",
    "Before retrieving sensor parameters, authenticate with Google Earth Engine running the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T12:49:56.028352Z",
     "iopub.status.busy": "2025-06-23T12:49:56.028072Z",
     "iopub.status.idle": "2025-06-23T12:50:02.547536Z",
     "shell.execute_reply": "2025-06-23T12:50:02.546549Z",
     "shell.execute_reply.started": "2025-06-23T12:49:56.028329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# auth.py\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import json\n",
    "import ee\n",
    "\n",
    "# Get the Service Account key from Kaggle Secrets\n",
    "user_secrets = UserSecretsClient()\n",
    "gcloud_key = user_secrets.get_secret(\"service_account\")  # or the secret name you used\n",
    "\n",
    "# Save the key to a file (Earth Engine expects a file)\n",
    "with open('gcloud_key.json', 'w') as f:\n",
    "    f.write(gcloud_key)\n",
    "\n",
    "# Load the service account email\n",
    "service_account_info = json.loads(gcloud_key)\n",
    "service_account = service_account_info['client_email']\n",
    "\n",
    "# Authenticate Earth Engine with the service account\n",
    "credentials = ee.ServiceAccountCredentials(service_account, 'gcloud_key.json')\n",
    "ee.Initialize(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With authentication in place, I can now programmatically retrieve a rich suite of environmental parameters for each benchmark site. These include NDVI (vegetation index), NDWI (surface water), NDBI (bare soil and built-up areas), SRTM elevation, terrain slope, Sentinel-1 radar backscatter (VV and VH polarizations), MapBiomas land cover classification, and GEDI-derived canopy height where available. This multi-sensor approach allows for a more nuanced characterization of each site and provides a robust reference framework for detecting potential anomalies across the Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "execution_failed": "2025-06-23T13:29:58.281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get-benchmark-data.py\n",
    "\n",
    "# --- DATASET IDS USED ---\n",
    "DATASET_IDS = [\n",
    "    'COPERNICUS/S2_SR_HARMONIZED',\n",
    "    'COPERNICUS/S1_GRD',\n",
    "    'USGS/SRTMGL1_003',\n",
    "    'projects/mapbiomas-raisg/public/collection3/mapbiomas_raisg_panamazonia_collection3_integration_v2',\n",
    "    'LARSE/GEDI/GEDI02_A_002_MONTHLY',\n",
    "    'NASA/JPL/global_forest_canopy_height_2005'\n",
    "]\n",
    "\n",
    "print(\"[INFO] Datasets used in this script:\")\n",
    "for ds in DATASET_IDS:\n",
    "    print(f\"  - {ds}\")\n",
    "print()\n",
    "\n",
    "# --- Earth Engine functions ---\n",
    "# --- Authenticate with Earth Engine before running this block ---\n",
    "\n",
    "import ee\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# --- Earth Engine functions ---\n",
    "\n",
    "def get_ndvi(lat, lon, year=2023, buffer_m=50):\n",
    "    point = ee.Geometry.Point([lon, lat]).buffer(buffer_m)\n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterBounds(point)\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "          .map(lambda img: img.normalizedDifference(['B8', 'B4']).rename('NDVI')))\n",
    "    ndvi = s2.median().select('NDVI').reduceRegion(\n",
    "        reducer=ee.Reducer.mean(), geometry=point, scale=10).get('NDVI')\n",
    "    return ndvi.getInfo() if ndvi is not None else None\n",
    "\n",
    "def get_ndwi(lat, lon, year=2023, buffer_m=50):\n",
    "    point = ee.Geometry.Point([lon, lat]).buffer(buffer_m)\n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterBounds(point)\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "          .map(lambda img: img.normalizedDifference(['B3', 'B8']).rename('NDWI')))\n",
    "    ndwi = s2.median().select('NDWI').reduceRegion(\n",
    "        reducer=ee.Reducer.mean(), geometry=point, scale=10).get('NDWI')\n",
    "    return ndwi.getInfo() if ndwi is not None else None\n",
    "\n",
    "def get_ndbi(lat, lon, year=2023, buffer_m=50):\n",
    "    point = ee.Geometry.Point([lon, lat]).buffer(buffer_m)\n",
    "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "          .filterBounds(point)\n",
    "          .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "          .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "          .map(lambda img: img.normalizedDifference(['B11', 'B8']).rename('NDBI')))\n",
    "    ndbi = s2.median().select('NDBI').reduceRegion(\n",
    "        reducer=ee.Reducer.mean(), geometry=point, scale=10).get('NDBI')\n",
    "    return ndbi.getInfo() if ndbi is not None else None\n",
    "\n",
    "def get_srtm_elevation(lat, lon, buffer_m=50):\n",
    "    point = ee.Geometry.Point([lon, lat]).buffer(buffer_m)\n",
    "    srtm = ee.Image(\"USGS/SRTMGL1_003\")\n",
    "    elev = srtm.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(), geometry=point, scale=30).get('elevation')\n",
    "    return elev.getInfo() if elev is not None else None\n",
    "\n",
    "def get_srtm_slope(lat, lon, buffer_m=50):\n",
    "    point = ee.Geometry.Point([lon, lat]).buffer(buffer_m)\n",
    "    elev = ee.Image(\"USGS/SRTMGL1_003\")\n",
    "    slope = ee.Terrain.slope(elev)\n",
    "    slope_val = slope.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(), geometry=point, scale=30).get('slope')\n",
    "    return slope_val.getInfo() if slope_val is not None else None\n",
    "\n",
    "def get_sentinel1_vv(lat, lon, year=2023, buffer_m=1000):\n",
    "    \"\"\"\n",
    "    Returns the average VV backscatter of Sentinel-1.\n",
    "    \n",
    "    buffer_m = 1000 by default (>> 50 m from other sensors)\n",
    "    ──────────────────────────────────────────────────────────\n",
    "    Using a larger buffer reduces the speckle noise characteristic\n",
    "    of radar images when spatially averaging. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        point = ee.Geometry.Point(lon, lat)\n",
    "        roi = point.buffer(buffer_m).bounds()\n",
    "        s1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "            .filterBounds(roi) \\\n",
    "            .filterDate(f'{year}-01-01', f'{year}-12-31') \\\n",
    "            .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "            .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
    "            .select('VV')\n",
    "        count = s1.size().getInfo()\n",
    "        if count == 0:\n",
    "            return None\n",
    "        s1_img = s1.median()\n",
    "        vv_value = s1_img.reduceRegion(ee.Reducer.mean(), point, 30).get('VV')\n",
    "        return vv_value.getInfo() if vv_value is not None else None\n",
    "    except Exception as e:\n",
    "        print(f\"Sentinel-1 VV error at ({lat}, {lon}): {e}\")\n",
    "        return None\n",
    "\n",
    "def get_sentinel1_vh(lat, lon, year=2023, buffer_m=1000):\n",
    "    \"\"\"\n",
    "    Returns the average VH backscatter from Sentinel-1.\n",
    "    \n",
    "    The 1000m buffer helps to smooth out radar speckle.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        point = ee.Geometry.Point(lon, lat)\n",
    "        roi = point.buffer(buffer_m).bounds()\n",
    "        s1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "            .filterBounds(roi) \\\n",
    "            .filterDate(f'{year}-01-01', f'{year}-12-31') \\\n",
    "            .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "            .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH')) \\\n",
    "            .select('VH')\n",
    "        count = s1.size().getInfo()\n",
    "        if count == 0:\n",
    "            return None\n",
    "        s1_img = s1.median()\n",
    "        vh_value = s1_img.reduceRegion(ee.Reducer.mean(), point, 30).get('VH')\n",
    "        return vh_value.getInfo() if vh_value is not None else None\n",
    "    except Exception as e:\n",
    "        print(f\"Sentinel-1 VH error at ({lat}, {lon}): {e}\")\n",
    "        return None\n",
    "\n",
    "def get_mapbiomas_class(lat, lon, year=2020):\n",
    "    try:\n",
    "        point = ee.Geometry.Point(lon, lat)\n",
    "        img = ee.Image('projects/mapbiomas-raisg/public/collection3/mapbiomas_raisg_panamazonia_collection3_integration_v2') \\\n",
    "            .select(f'classification_{year}')\n",
    "        value = img.reduceRegion(ee.Reducer.mode(), point, 30).get(f'classification_{year}')\n",
    "        return value.getInfo() if value is not None else None\n",
    "    except Exception as e:\n",
    "        print(f\"MapBiomas error at ({lat}, {lon}): {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_gedi_canopy_height(lat, lon):\n",
    "    \"\"\"\n",
    "    Returns mean GEDI canopy height (rh98) for a point.\n",
    "    If GEDI is not available, fallback to NASA/JPL/global_forest_canopy_height_2005.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        point = ee.Geometry.Point([lon, lat])\n",
    "        gedi = (ee.ImageCollection('LARSE/GEDI/GEDI02_A_002_MONTHLY')\n",
    "                .filterBounds(point))\n",
    "        if gedi.size().getInfo() == 0:\n",
    "            raise ValueError(\"No GEDI pulses\")\n",
    "        gedi_img = gedi.select('rh98').median()\n",
    "        value = gedi_img.reduceRegion(\n",
    "            ee.Reducer.mean(), point, 25).get('rh98')\n",
    "        val = value.getInfo() if value is not None else None\n",
    "        if val is not None and val != 0:\n",
    "            return val\n",
    "        # If value is None or 0, fallback\n",
    "        raise ValueError(\"GEDI returned None or 0\")\n",
    "    except Exception as e:\n",
    "        print(f\"GEDI error at ({lat}, {lon}): {e}. Trying NASA/JPL/global_forest_canopy_height_2005...\")\n",
    "        try:\n",
    "            # NASA/JPL/global_forest_canopy_height_2005: altura média do dossel em metros (2005)\n",
    "            point = ee.Geometry.Point([lon, lat])\n",
    "            canopy_img = ee.Image('NASA/JPL/global_forest_canopy_height_2005')\n",
    "            value = canopy_img.reduceRegion(\n",
    "                ee.Reducer.mean(), point, 1000).get('1')\n",
    "            val = value.getInfo() if value is not None else None\n",
    "            return val\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback canopy height error at ({lat}, {lon}): {e2}\")\n",
    "            return None\n",
    "\n",
    "# --- Enrich DataFrame with all sensors ---\n",
    "\n",
    "def enrich_benchmarks_with_all_sensors(\n",
    "    df,\n",
    "    ndvi_year=2023,\n",
    "    ndwi_year=2023,\n",
    "    ndbi_year=2023,\n",
    "    s1_year=2023,\n",
    "    mapbiomas_year=2020,\n",
    "    buffer_m=50,\n",
    "    delay=1\n",
    "):\n",
    "    ndvi_list = []\n",
    "    ndwi_list = []\n",
    "    ndbi_list = []\n",
    "    elev_list = []\n",
    "    slope_list = []\n",
    "    vv_list = []\n",
    "    vh_list = []\n",
    "    landclass_list = []\n",
    "    canopyheight_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        lat, lon = row['lat'], row['lon']\n",
    "        print(f\"Processing {row.get('name', 'site')} ({lat}, {lon})...\")\n",
    "        ndvi_list.append(get_ndvi(lat, lon, ndvi_year, buffer_m))\n",
    "        ndwi_list.append(get_ndwi(lat, lon, ndwi_year, buffer_m))\n",
    "        ndbi_list.append(get_ndbi(lat, lon, ndbi_year, buffer_m))\n",
    "        elev_list.append(get_srtm_elevation(lat, lon, buffer_m))\n",
    "        slope_list.append(get_srtm_slope(lat, lon, buffer_m))\n",
    "        vv_list.append(get_sentinel1_vv(lat, lon, s1_year, buffer_m=1000))\n",
    "        vh_list.append(get_sentinel1_vh(lat, lon, s1_year, buffer_m=1000))\n",
    "        landclass_list.append(get_mapbiomas_class(lat, lon, mapbiomas_year))\n",
    "        canopyheight_list.append(get_gedi_canopy_height(lat, lon))\n",
    "        time.sleep(delay)  # To avoid quota limits\n",
    "\n",
    "    df['NDVI'] = ndvi_list\n",
    "    df['NDWI'] = ndwi_list\n",
    "    df['NDBI'] = ndbi_list\n",
    "    df['Elevation'] = elev_list\n",
    "    df['Slope'] = slope_list\n",
    "    df['Sentinel1_VV'] = vv_list\n",
    "    df['Sentinel1_VH'] = vh_list\n",
    "    df['MapBiomas_Class'] = landclass_list\n",
    "    df['CanopyHeight'] = canopyheight_list\n",
    "    return df\n",
    "\n",
    "# --- Usage example ---\n",
    "# df_benchmark = pd.read_csv(\"benchmark_sites_acre.csv\")  # or from previous cell\n",
    "df_benchmark = enrich_benchmarks_with_all_sensors(df_benchmark)\n",
    "\n",
    "# Substitui infinitos por NaN para evitar warnings do pandas\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "df_benchmark.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "display(df_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Data to Discovery\n",
    "With my virtual backpack ready—AI-generated benchmarks, authenticated Earth Engine access, and a toolkit of remote sensing algorithms—I set out to hunt for secrets hidden beneath the Amazonian canopy. NDVI would reveal the scars of ancient presence. SRTM would whisper the contours of earthworks and platforms lost to time. Sentinel-1 would pierce the clouds with radar eyes, while MapBiomas would map the shifting mosaic of land cover.\n",
    "\n",
    "But the journey was just beginning. Now, to push deeper into the unknown, I called on AI to help identify the most promising and mysterious places—regions along the legendary Nhamini-wi trail where stories are plenty, but archaeological exploration remains scarce. This search would not be made on foot, but pixel by pixel, parameter by parameter, legend by legend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-23T13:29:58.299Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# search-candidates.py\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize OpenAI client\n",
    "user_secrets = UserSecretsClient()\n",
    "client = openai.OpenAI(api_key=user_secrets.get_secret(\"openai\"))\n",
    "\n",
    "# Prompt: ask o3 for promising but underexplored locations in Nhamini-wi territories (≤200 chars rationale)\n",
    "prompt = (\n",
    "    \"You are an Amazon explorer and researcher.\\n\"\n",
    "    \"Based on historical legends, indigenous oral history, and published expedition records, \"\n",
    "    \"suggest up to 5 possible locations (latitude and longitude) within the Nhamini-wi region (Upper Rio Negro, near the Brazil/Colombia/Venezuela border) \"\n",
    "    \"that could correspond to the legendary trail or its unexplored sites. \"\n",
    "    \"Focus on areas that remain little explored archaeologically, according to the scientific literature. \"\n",
    "    \"For each, briefly justify your choice referencing myths, remoteness, or lack of fieldwork. \"\n",
    "    \"Return your answer as a JSON list with the fields: name, lat, lon, rationale (≤200 characters), and radius_m (fixed value, e.g., 500). \"\n",
    "    \"Example: [{\\\"name\\\": \\\"Suggested Area\\\", \\\"lat\\\": 1.2345, \\\"lon\\\": -67.8901, \\\"rationale\\\": \\\"...\\\", \\\"radius_m\\\": 500}, ...]\"\n",
    ")\n",
    "\n",
    "# Define schema with Pydantic (now includes radius_m)\n",
    "class Area(BaseModel):\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    rationale: str\n",
    "    radius_m: int = 500  # default radius in meters\n",
    "\n",
    "class SuggestedAreas(BaseModel):\n",
    "    areas: List[Area]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"o3\",\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    text_format=SuggestedAreas,\n",
    ")\n",
    "\n",
    "areas = response.output_parsed.areas\n",
    "\n",
    "# Ensure full text is shown in the 'rationale' column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# Helper functions for bbox and WKT\n",
    "import math\n",
    "def get_bbox(lat, lon, radius_m):\n",
    "    # Approximate 1 degree latitude ~ 111.32 km, longitude varies with latitude\n",
    "    dlat = (radius_m / 111320)\n",
    "    dlon = (radius_m / (40075000 * math.cos(math.radians(lat)) / 360))\n",
    "    return [lon - dlon, lat - dlat, lon + dlon, lat + dlat]\n",
    "\n",
    "def get_bbox_wkt(bbox):\n",
    "    min_lon, min_lat, max_lon, max_lat = bbox\n",
    "    return (\n",
    "        f\"POLYGON((\"\n",
    "        f\"{min_lon} {min_lat}, \"\n",
    "        f\"{min_lon} {max_lat}, \"\n",
    "        f\"{max_lon} {max_lat}, \"\n",
    "        f\"{max_lon} {min_lat}, \"\n",
    "        f\"{min_lon} {min_lat}\"  # close polygon\n",
    "        f\"))\"\n",
    "    )\n",
    "\n",
    "def get_circle_wkt(lat, lon, radius_m, n_points=36):\n",
    "    # Approximate circle as polygon\n",
    "    coords = []\n",
    "    for i in range(n_points+1):\n",
    "        angle = 2 * math.pi * i / n_points\n",
    "        dlat = (radius_m / 111320) * math.sin(angle)\n",
    "        dlon = (radius_m / (40075000 * math.cos(math.radians(lat)) / 360)) * math.cos(angle)\n",
    "        coords.append(f\"{lon + dlon} {lat + dlat}\")\n",
    "    return f\"POLYGON(({', '.join(coords)}))\"\n",
    "\n",
    "# Monta DataFrame com bbox e WKT\n",
    "df = pd.DataFrame([a.model_dump() for a in areas])\n",
    "df['bbox'] = df.apply(lambda row: get_bbox(row['lat'], row['lon'], row['radius_m']), axis=1)\n",
    "df['bbox_wkt'] = df['bbox'].apply(get_bbox_wkt)\n",
    "df['circle_wkt'] = df.apply(lambda row: get_circle_wkt(row['lat'], row['lon'], row['radius_m']), axis=1)\n",
    "\n",
    "\n",
    "# Display as DataFrame (organized for notebook/Kaggle, agora inclui bbox e WKT)\n",
    "display(df[['name', 'lat', 'lon', 'radius_m', 'bbox', 'bbox_wkt', 'circle_wkt', 'rationale']].rename(columns={\n",
    "    'name': 'Name',\n",
    "    'lat': 'Latitude',\n",
    "    'lon': 'Longitude',\n",
    "    'radius_m': 'Radius (m)',\n",
    "    'bbox': 'BBox [min_lon, min_lat, max_lon, max_lat]',\n",
    "    'bbox_wkt': 'BBox WKT',\n",
    "    'circle_wkt': 'Circle WKT',\n",
    "    'rationale': 'Rationale (≤200 chars)'\n",
    "}))\n",
    "\n",
    "# Print model version used\n",
    "print(f\"\\n[INFO] OpenAI model used: o3\")\n",
    "\n",
    "# Print coordinates and radius for reference\n",
    "print(\"\\nSuggested Coordinates (with radius):\")\n",
    "for area in areas:\n",
    "    print(f\"{area.name}: lat {area.lat}, lon {area.lon}, radius {area.radius_m}m\")\n",
    "\n",
    "# Display usage information safely\n",
    "usage = response.usage\n",
    "try:\n",
    "    print(\"\\nPrompt tokens:\", getattr(usage, \"prompt_tokens\", None))\n",
    "    print(\"Completion tokens:\", getattr(usage, \"completion_tokens\", None))\n",
    "    print(\"Total tokens:\", getattr(usage, \"total_tokens\", None))\n",
    "except Exception:\n",
    "    print(\"\\nUsage info:\", usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensing the Unknown\n",
    "\n",
    "Before drawing any conclusions, it was essential to peer beneath the forest cover at each of these newly suggested locations. Using the same multi-sensor approach as before, I extracted NDVI, NDWI, SRTM elevation, slope, Sentinel-1 radar backscatter, land cover, and, where available, canopy height. Each number became a digital footprint, a clue in the search for ancient paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-23T13:29:58.301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get-candidates-data.py\n",
    "\n",
    "# Run sensor enrichment on the new candidate areas\n",
    "# The column 'CanopyHeight' is used for both GEDI and NASA/JPL fallback, matching the benchmark structure\n",
    "\n",
    "# Log dataset IDs used for enrichment\n",
    "DATASET_IDS = [\n",
    "    'COPERNICUS/S2_SR_HARMONIZED',\n",
    "    'COPERNICUS/S1_GRD',\n",
    "    'USGS/SRTMGL1_003',\n",
    "    'projects/mapbiomas-raisg/public/collection3/mapbiomas_raisg_panamazonia_collection3_integration_v2',\n",
    "    'LARSE/GEDI/GEDI02_A_002_MONTHLY',\n",
    "    'NASA/JPL/global_forest_canopy_height_2005'\n",
    "]\n",
    "\n",
    "print(\"[INFO] Datasets used in candidate enrichment:\")\n",
    "for ds in DATASET_IDS:\n",
    "    print(f\"  - {ds}\")\n",
    "print()\n",
    "\n",
    "df_candidates = pd.DataFrame([a.model_dump() for a in areas])\n",
    "num_areas = len(areas)\n",
    "df_candidates = enrich_benchmarks_with_all_sensors(df_candidates)\n",
    "\n",
    "# Add Sentinel-2 thumbnail download column for each candidate\n",
    "import ee\n",
    "\n",
    "# Functions to generate download links for different sensors\n",
    "def get_rgb_download_url_html(lat, lon, year=\"2023\", month=\"05\"):\n",
    "    try:\n",
    "        collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(ee.Geometry.Point(lon, lat)) \\\n",
    "            .filterDate(f'{year}-{month}-01', f'{year}-{month}-31')\n",
    "        image = collection.first()\n",
    "        region = ee.Geometry.Point(lon, lat).buffer(500).bounds()\n",
    "        url = image.getThumbURL({\n",
    "            'bands': ['B4', 'B3', 'B2'],\n",
    "            'min': 500, 'max': 2500,\n",
    "            'dimensions': 512,\n",
    "            'region': region\n",
    "        })\n",
    "        if url:\n",
    "            return f'<a href=\"{url}\" target=\"_blank\">RGB</a>'\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_ndvi_download_url_html(lat, lon, year=\"2023\", month=\"05\"):\n",
    "    try:\n",
    "        collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(ee.Geometry.Point(lon, lat)) \\\n",
    "            .filterDate(f'{year}-{month}-01', f'{year}-{month}-31')\n",
    "        image = collection.first().normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "        region = ee.Geometry.Point(lon, lat).buffer(500).bounds()\n",
    "        url = image.getThumbURL({\n",
    "            'min': 0, 'max': 1,\n",
    "            'palette': ['blue', 'white', 'green'],\n",
    "            'dimensions': 512,\n",
    "            'region': region\n",
    "        })\n",
    "        if url:\n",
    "            return f'<a href=\"{url}\" target=\"_blank\">NDVI</a>'\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_ndwi_download_url_html(lat, lon, year=\"2023\", month=\"05\"):\n",
    "    try:\n",
    "        collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(ee.Geometry.Point(lon, lat)) \\\n",
    "            .filterDate(f'{year}-{month}-01', f'{year}-{month}-31')\n",
    "        image = collection.first().normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
    "        region = ee.Geometry.Point(lon, lat).buffer(500).bounds()\n",
    "        url = image.getThumbURL({\n",
    "            'min': -1, 'max': 1,\n",
    "            'palette': ['brown', 'beige', 'blue'],\n",
    "            'dimensions': 512,\n",
    "            'region': region\n",
    "        })\n",
    "        if url:\n",
    "            return f'<a href=\"{url}\" target=\"_blank\">NDWI</a>'\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_ndbi_download_url_html(lat, lon, year=\"2023\", month=\"05\"):\n",
    "    try:\n",
    "        collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "            .filterBounds(ee.Geometry.Point(lon, lat)) \\\n",
    "            .filterDate(f'{year}-{month}-01', f'{year}-{month}-31')\n",
    "        image = collection.first().normalizedDifference(['B11', 'B8']).rename('NDBI')\n",
    "        region = ee.Geometry.Point(lon, lat).buffer(500).bounds()\n",
    "        url = image.getThumbURL({\n",
    "            'min': -1, 'max': 1,\n",
    "            'palette': ['white', 'gray', 'black'],\n",
    "            'dimensions': 512,\n",
    "            'region': region\n",
    "        })\n",
    "        if url:\n",
    "            return f'<a href=\"{url}\" target=\"_blank\">NDBI</a>'\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_s1_vv_download_url_html(lat, lon, year=\"2023\"):\n",
    "    try:\n",
    "        point = ee.Geometry.Point(lon, lat).buffer(500)\n",
    "        s1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "            .filterBounds(point) \\\n",
    "            .filterDate(f'{year}-01-01', f'{year}-12-31') \\\n",
    "            .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "            .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
    "            .select('VV')\n",
    "        s1_img = s1.median().clip(point)\n",
    "        url = s1_img.getThumbURL({\n",
    "            'region': point, 'dimensions': 512, 'min': -25, 'max': 0,\n",
    "            'palette': ['black', 'white']})\n",
    "        if url:\n",
    "            return f'<a href=\"{url}\" target=\"_blank\">Sentinel-1 VV</a>'\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Download column with all available links (adds only the sensors that are actually available)\n",
    "def make_download_links(row):\n",
    "    links = []\n",
    "    rgb = get_rgb_download_url_html(row['lat'], row['lon'])\n",
    "    if rgb:\n",
    "        links.append(rgb)\n",
    "    ndvi = get_ndvi_download_url_html(row['lat'], row['lon'])\n",
    "    if ndvi:\n",
    "        links.append(ndvi)\n",
    "    ndwi = get_ndwi_download_url_html(row['lat'], row['lon'])\n",
    "    if ndwi:\n",
    "        links.append(ndwi)\n",
    "    ndbi = get_ndbi_download_url_html(row['lat'], row['lon'])\n",
    "    if ndbi:\n",
    "        links.append(ndbi)\n",
    "    s1vv = get_s1_vv_download_url_html(row['lat'], row['lon'])\n",
    "    if s1vv:\n",
    "        links.append(s1vv)\n",
    "    # Add other sensors here if needed\n",
    "    return ' | '.join(links)\n",
    "\n",
    "df_candidates['Download'] = df_candidates.apply(make_download_links, axis=1)\n",
    "\n",
    "# For notebooks: display HTML correctly\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(df_candidates.to_html(escape=False)))\n",
    "\n",
    "# Check: ensure all candidates are present after enrichment\n",
    "if len(df_candidates) != num_areas:\n",
    "    print(f\"[ERROR] Expected {num_areas} candidates, but df_candidates has {len(df_candidates)} after enrichment!\")\n",
    "    print(\"Expected IDs:\", [a['name'] if hasattr(a, 'name') else a.get('name') for a in areas])\n",
    "    print(\"Present IDs:\", df_candidates['name'].tolist() if 'name' in df_candidates.columns else df_candidates.index.tolist())\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Legends and Landmarks\n",
    "\n",
    "With sensor profiles in hand for both the benchmark sites and the candidate Nhamini-wi locations, the next step was comparison. How do the physical and environmental signatures of these unexplored spots match up against those of confirmed archaeological sites? Are there echoes—statistical or spectral—between legend and proven earthwork?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-23T13:29:58.302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# compare.py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sensor_cols = [col for col in df_benchmark.columns if df_benchmark[col].dtype != object]\n",
    "\n",
    "# Select only sensors with valid values in at least one group\n",
    "valid_cols = [\n",
    "    col for col in sensor_cols\n",
    "    if (df_benchmark[col].notna().any() and df_candidates[col].notna().any())\n",
    "]\n",
    "\n",
    "coverage = (df_candidates[valid_cols].notna().sum() / len(df_candidates)).round(2)\n",
    "print(\"Proportion of valid values ​​in each sensor (candidates):\")\n",
    "print(coverage)\n",
    "\n",
    "# --- Explicit comparison explanation ---\n",
    "print(\"\\n[INFO] The following plot and statistics provide a direct comparison between the environmental parameters of known archaeological benchmarks and the new candidates.\\n\"\n",
    "      \"The Z-score profile plot visualizes how similar or different the candidates are from the benchmarks for each sensor.\\n\"\n",
    "      \"Use this to identify which candidates most closely resemble known sites, or which parameters stand out as anomalous.\")\n",
    "\n",
    "z_bench = (df_benchmark[valid_cols] - df_benchmark[valid_cols].mean()) \\\n",
    "          / df_benchmark[valid_cols].std()\n",
    "\n",
    "z_cand  = (df_candidates[valid_cols] - df_benchmark[valid_cols].mean()) \\\n",
    "          / df_benchmark[valid_cols].std()\n",
    "\n",
    "means_bench = z_bench.mean().values\n",
    "means_cand  = z_cand.mean().values\n",
    "\n",
    "x = np.arange(len(valid_cols))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x, means_bench, marker='o', label='Benchmark (z)', linewidth=2)\n",
    "ax.plot(x, means_cand, marker='s', label='Candidates (z)', linewidth=2)\n",
    "\n",
    "ax.set_ylabel(\"Z-score (σ)\")\n",
    "ax.set_title('Sensor Parameter Means Profile: Benchmarks vs. Nhamini-wi Candidates')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(valid_cols, rotation=45, ha='right')\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n",
    "ax.legend()\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Assessment: Insights from the Digital Expedition\n",
    "\n",
    "Armed with all these layers of data, I turned again to AI—not just to propose locations, but to evaluate the evidence. Feeding the summarized results and sensor comparisons into the model, I asked for an expert assessment:\n",
    "Do any of these Nhamini-wi areas stand out as genuine archaeological anomalies? Where does the digital evidence most closely echo the marks of ancient hands?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-23T13:29:58.303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# analyze-candidates-data.py\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    openai_key = user_secrets.get_secret(\"openai\")\n",
    "except Exception:\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "from pydantic import BaseModel\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except ImportError:\n",
    "    display = None\n",
    "\n",
    "# Generates the mean summary for all benchmark sensors\n",
    "def generate_sensor_summary(df, label):\n",
    "    lines = [f\"{label} stats (mean):\"]\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != object:\n",
    "            val = df[col].mean()\n",
    "            lines.append(f\"{col}: {val:.3f}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Generates a detailed summary for all candidates\n",
    "def generate_candidates_detail(df):\n",
    "    lines = [\"Candidates:\"]\n",
    "    for idx, row in df.iterrows():\n",
    "        vals = []\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype != object:\n",
    "                vals.append(f\"{col}: {row[col]:.3f}\")\n",
    "            else:\n",
    "                vals.append(f\"{col}: {row[col]}\")\n",
    "        lines.append(\"- \" + \", \".join(vals))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "summary_bench = generate_sensor_summary(df_benchmark, \"Benchmark\")\n",
    "summary_cand = generate_candidates_detail(df_candidates)\n",
    "summary = f\"{summary_bench}\\n\\n{summary_cand}\"\n",
    "\n",
    "prompt = (\n",
    "    \"You are an expert in Amazonian remote sensing and archaeology.\\n\"\n",
    "    \"Below are summarized environmental parameters for known archaeological sites (benchmarks) and for new candidate locations along the Nhamini-wi trail.\\n\"\n",
    "    \"Based on this data, compare the candidates to the benchmarks and assess:\\n\"\n",
    "    \"- Which, if any, of the candidates most closely match the benchmarks?\\n\"\n",
    "    \"- Are there anomalies or promising signals in the candidate data that warrant field investigation?\\n\"\n",
    "    \"- Briefly explain the key differences and what they might mean archaeologically.\\n\"\n",
    "    \"Be concise and analytical, referencing the key parameters (NDVI, NDWI, NDBI, SRTM, slope, Sentinel-1 radar, land cover, canopy height, etc.).\\n\"\n",
    "    \"Return ONLY a JSON object with a 'matches' key, which is a list of the closest candidate(s) to the benchmarks. Each match must have: name, lat, lon, reason. If none, return an empty list.\\n\"\n",
    "    f\"\\n{summary}\\n\"\n",
    ")\n",
    "    # Defines the schema for Structured Outputs\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"matches\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\":   {\"type\": \"string\"},\n",
    "                    \"lat\":    {\"type\": \"number\"},\n",
    "                    \"lon\":    {\"type\": \"number\"},\n",
    "                    \"reason\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"name\", \"lat\", \"lon\", \"reason\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"matches\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "# --- Structured Output with Pydantic ---\n",
    "client = OpenAI(api_key=openai_key) if openai_key else OpenAI()\n",
    "\n",
    "class ClosestMatch(BaseModel):\n",
    "    name: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    reason: str\n",
    "\n",
    "class ClosestMatches(BaseModel):\n",
    "    matches: list[ClosestMatch]\n",
    "\n",
    "model_name = \"o3\"\n",
    "response = client.responses.parse(\n",
    "    model=model_name,\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    text_format=ClosestMatches,\n",
    ")\n",
    "\n",
    "matches = response.output_parsed.matches\n",
    "\n",
    "# Print model version used\n",
    "print(f\"\\n[INFO] OpenAI model used: {model_name}\")\n",
    "\n",
    "print(\"\\nMatches:\")\n",
    "for m in matches:\n",
    "    print(f\"- {m.name} (lat: {m.lat}, lon: {m.lon})\\n  Reason: {m.reason}\\n\")\n",
    "\n",
    "# Display the result as a DataFrame in Kaggle/notebook environments (optional)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure full text is shown in the 'reason' column (rationale)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_matches = pd.DataFrame([m.model_dump() for m in matches])\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_matches)\n",
    "except Exception:\n",
    "    print(df_matches)\n",
    "\n",
    "# Display usage information safely (as in search-candidates.py)\n",
    "usage = getattr(response, \"usage\", None)\n",
    "try:\n",
    "    print(\"\\nPrompt tokens:\", getattr(usage, \"prompt_tokens\", getattr(usage, \"input_tokens\", None)))\n",
    "    print(\"Completion tokens:\", getattr(usage, \"completion_tokens\", getattr(usage, \"output_tokens\", None)))\n",
    "    print(\"Total tokens:\", getattr(usage, \"total_tokens\", None))\n",
    "except Exception:\n",
    "    print(\"\\nUsage info:\", usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the Unseen: Visualizing the Candidate Site\n",
    "\n",
    "Numbers and parameters told part of the story, but satellite images allow us to see the landscape much as an explorer might—from above, with the forest and rivers laid out like clues on a map. To better understand the chosen candidate site, I generated a series of satellite visualizations, each revealing a different aspect of the terrain.\n",
    "\n",
    "True-color imagery offers a window into the present-day canopy, while infrared and NDVI composites highlight patterns of vegetation and possible traces of past human activity. Water indices (NDWI) expose subtle changes in soil moisture, and radar imagery from Sentinel-1 allows us to pierce through clouds, mapping surface structure even during the Amazon’s rainy season.\n",
    "\n",
    "By layering these distinct “views from above,” I could build a richer picture of what might lie hidden beneath the trees—bringing together science, legend, and the raw power of remote sensing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-23T13:29:58.305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get-image-for-closest-match.py\n",
    "\n",
    "def plot_multiple_satellite_views(lat, lon, buffer_m=1000, year=2023):\n",
    "    point = ee.Geometry.Point(lon, lat).buffer(buffer_m)\n",
    "    print(\"[INFO] Using dataset_id: COPERNICUS/S2_SR_HARMONIZED (Sentinel-2) for RGB, Infrared (NIR), NDVI, NDWI\")\n",
    "    img = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "           .filterBounds(point)\n",
    "           .filterDate(f'{year}-01-01', f'{year}-12-31')\n",
    "           .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n",
    "           .median().clip(point))\n",
    "    # URLs for each composite\n",
    "    urls = {}\n",
    "    urls['RGB'] = img.select(['B4', 'B3', 'B2']).getThumbURL({\n",
    "        'region': point, 'dimensions': 512, 'min': 500, 'max': 2500})\n",
    "    urls['Infrared (NIR)'] = img.select(['B8', 'B4', 'B3']).getThumbURL({\n",
    "        'region': point, 'dimensions': 512, 'min': 500, 'max': 2500})\n",
    "    ndvi = img.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    urls['NDVI'] = ndvi.getThumbURL({\n",
    "        'region': point, 'dimensions': 512, 'min': 0, 'max': 1,\n",
    "        'palette': ['blue', 'white', 'green']})\n",
    "    ndwi = img.normalizedDifference(['B3', 'B8']).rename('NDWI')\n",
    "    urls['NDWI'] = ndwi.getThumbURL({\n",
    "        'region': point, 'dimensions': 512, 'min': -1, 'max': 1,\n",
    "        'palette': ['brown', 'beige', 'blue']})\n",
    "    print(\"[INFO] Using dataset_id: COPERNICUS/S1_GRD (Sentinel-1) for Sentinel-1 VV\")\n",
    "    # Sentinel-1 VV\n",
    "    s1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "        .filterBounds(point) \\\n",
    "        .filterDate(f'{year}-01-01', f'{year}-12-31') \\\n",
    "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
    "        .select('VV')\n",
    "    s1_img = s1.median().clip(point)\n",
    "    urls['Sentinel-1 VV'] = s1_img.getThumbURL({\n",
    "        'region': point, 'dimensions': 512, 'min': -25, 'max': 0,\n",
    "        'palette': ['black', 'white']})\n",
    "    # Plot all images\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for i, (name, url) in enumerate(urls.items()):\n",
    "        response = requests.get(url)\n",
    "        im = Image.open(BytesIO(response.content))\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(im)\n",
    "        plt.title(name)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage for all matches using the in-memory df_matches DataFrame:\n",
    "import pandas as pd\n",
    "# Log dataset ID if available in df_matches\n",
    "if 'df_matches' in globals() and df_matches is not None and not df_matches.empty:\n",
    "    dataset_id = None\n",
    "    # Try to get dataset_id from DataFrame attribute or column\n",
    "    if hasattr(df_matches, 'dataset_id'):\n",
    "        dataset_id = getattr(df_matches, 'dataset_id', None)\n",
    "    elif 'dataset_id' in df_matches.columns:\n",
    "        dataset_id = df_matches['dataset_id'].iloc[0]\n",
    "    if dataset_id:\n",
    "        print(f\"[INFO] Using dataset_id: {dataset_id}\")\n",
    "    for _, m in df_matches.iterrows():\n",
    "        print(f\"\\n[INFO] Generating images for: {m['name']} (lat: {m['lat']}, lon: {m['lon']})\")\n",
    "        plot_multiple_satellite_views(m['lat'], m['lon'], buffer_m=1000, year=2023)\n",
    "else:\n",
    "    print(\"No match found to generate images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Candidate Analysis for Future Discoveries\n",
    "The candidate sites identified and analyzed in this notebook are not only useful for the current comparison with known archaeological benchmarks, but also serve as a valuable resource for future research and fieldwork.\n",
    "\n",
    "**How to use these results for future discoveries:**\n",
    "\n",
    "1. **Re-prompting and Iterative Analysis:**\n",
    "   - The list of candidate sites (with their environmental parameters and model rationales) can be used as input for new prompts or models, enabling iterative refinement and prioritization as new data or hypotheses emerge.\n",
    "3. **Integration with New Datasets:**\n",
    "   - As additional remote sensing layers or ground-truth data become available, these candidates can be re-evaluated or enriched, helping to identify new patterns or confirm archaeological potential.\n",
    "4. **Fieldwork Planning:**\n",
    "   - The coordinates, bounding boxes, and rationales for each candidate can directly inform field survey planning, allowing teams to focus on the most promising or anomalous locations.\n",
    "6. **Reproducibility and Collaboration:**\n",
    "   - By maintaining a structured, notebook-friendly DataFrame of candidates, all steps and decisions are transparent and reproducible, facilitating collaboration and further analysis by other researchers.\n",
    "\n",
    "**Tip:**\n",
    "You can always re-use the `df_matches` DataFrame as input for new analyses, visualizations, or as a filter for satellite image extraction and further model runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "### 1. Benchmark Site Generation (`benchmark.py`)\n",
    "- Uses OpenAI's o3 model to generate a list of known archaeological sites in Acre, Brazil\n",
    "- Focuses on geoglyphs and earthworks documented in academic literature\n",
    "- Creates a reference dataset with coordinates (latitude/longitude) for comparison\n",
    "- Outputs data in both CSV format and formatted tables\n",
    "  \n",
    "### 2. Authentication (`auth.py`)\n",
    "- Authenticates with Google Earth Engine using service account credentials\n",
    "- Retrieves API keys from Kaggle Secrets for security\n",
    "- Sets up the connection to Earth Engine services\n",
    "\n",
    "### 3. Remote Sensing Data Collection (`get-benchmark-data.py`)\n",
    "This is the core module that enriches location data with multiple satellite sensors:\n",
    "\n",
    "#### Environmental Parameters Collected:\n",
    "- **NDVI (Normalized Difference Vegetation Index)** - Vegetation health indicator\n",
    "- **NDWI (Normalized Difference Water Index)** - Water content and moisture\n",
    "- **NDBI (Normalized Difference Built-up Index)** - Built environment detection\n",
    "- **SRTM Elevation & Slope** - Topographical characteristics\n",
    "- **Sentinel-1 Radar (VV/VH)** - Penetrates vegetation to detect subsurface features\n",
    "- **MapBiomas Land Cover Classification** - Land use classification\n",
    "- **GEDI Canopy Height** - Forest canopy structure from LiDAR\n",
    "\n",
    "#### Data Sources:\n",
    "- Sentinel-2 optical imagery (Copernicus program)\n",
    "- Sentinel-1 synthetic aperture radar\n",
    "- SRTM digital elevation model\n",
    "- MapBiomas Pan-Amazon land cover data\n",
    "- GEDI (Global Ecosystem Dynamics Investigation) LiDAR\n",
    "\n",
    "### 4. Candidate Site Discovery (`search-candidates.py`)\n",
    "- Uses AI to suggest promising but underexplored locations in the Nhamini-wi region\n",
    "- Based on historical legends, indigenous oral history, and expedition records\n",
    "- Generates hypothetical coordinates for areas that warrant archaeological investigation\n",
    "- Provides rationale for each suggested location\n",
    "- **Each candidate footprint includes a center (latitude/longitude) and a fixed radius (e.g., 500m), allowing representation as a circle or bounding box (bbox/WKT) for spatial analysis, as required by the OpenAI to Z Challenge.**\n",
    "\n",
    "### 5. Candidate Data Processing (`get-candidates-data.py`)\n",
    "- Applies the same remote sensing analysis to candidate locations\n",
    "- Creates comparable datasets between known sites and potential discoveries\n",
    "\n",
    "### 6. Comparative Analysis (`compare.py`)\n",
    "- Performs statistical comparison between benchmark sites and candidate locations\n",
    "- Normalizes sensor data using z-scores for fair comparison\n",
    "- Creates visualization plots showing environmental parameter profiles\n",
    "- Identifies which candidates most closely match known archaeological sites\n",
    "\n",
    "### 7. AI-Powered Site Assessment (`analyze-candidates-data.py`)\n",
    "- Uses OpenAI's o3 model to analyze environmental data patterns\n",
    "- Compares candidate sites against benchmark archaeological sites\n",
    "- Provides expert-level interpretation of remote sensing anomalies\n",
    "- Outputs JSON-formatted results with closest matches and archaeological significance\n",
    "\n",
    "### 8. Satellite Imagery Visualization (`get-image-for-closest-match.py`)\n",
    "- Generates multi-spectral satellite views of promising locations\n",
    "- Creates composite images including:\n",
    "  - True color RGB\n",
    "  - Near-infrared false color\n",
    "  - NDVI vegetation index\n",
    "  - NDWI water index\n",
    "  - Sentinel-1 radar backscatter\n",
    "- Provides visual inspection capabilities for identified sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "### Remote Sensing Approach\n",
    "The project employs a multi-sensor approach to characterize archaeological sites:\n",
    "1. **Optical sensors** detect vegetation anomalies and surface features\n",
    "2. **Radar sensors** penetrate forest canopy to reveal subsurface structures\n",
    "3. **LiDAR data** provides precise elevation and canopy measurements\n",
    "4. **Land cover data** identifies human-modified landscapes\n",
    "\n",
    "### AI Integration\n",
    "- **Site Discovery**: AI generates hypotheses about unexplored locations based on historical and cultural knowledge\n",
    "- **Pattern Recognition**: Machine learning identifies environmental signatures of known archaeological sites\n",
    "- **Expert Analysis**: AI provides archaeological interpretation of remote sensing anomalies\n",
    "\n",
    "### Statistical Analysis\n",
    "- Z-score normalization enables comparison across different sensor types\n",
    "- Pattern matching identifies candidate sites with similar environmental signatures to known archaeological locations\n",
    "\n",
    "## Applications\n",
    "- **Archaeological Survey Planning**: Prioritizes areas for field investigation\n",
    "- **Cultural Heritage Protection**: Identifies sites at risk from deforestation or development\n",
    "- **Indigenous Territory Mapping**: Documents traditional landscapes and settlement patterns\n",
    "- **Environmental Archaeology**: Studies human-environment interactions in the Amazon\n",
    "\n",
    "## Technical Requirements\n",
    "- Google Earth Engine account with API access\n",
    "- OpenAI API key (o3 model access)\n",
    "- Python environment with geospatial libraries\n",
    "- Kaggle environment for secure credential management\n",
    "\n",
    "## Data Outputs\n",
    "- DataFrame with enriched location data\n",
    "- Comparative analysis plots\n",
    "- Satellite imagery composites\n",
    "- JSON reports with archaeological assessments\n",
    "- Prioritized lists of sites for field verification\n",
    "\n",
    "This project demonstrates the integration of artificial intelligence with remote sensing technology for archaeological discovery in one of the world's most challenging environments - the Amazon rainforest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Github project [link](https://github.com/giovannicocco/openai-z-challenge) with OpenAI Agent Example with Function Calling and Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Limitations and Next Steps\n",
    "> **Limitations:**\n",
    "- Dependence on open data and sensor coverage may limit the detection of micro-sites or structures under dense canopy.\n",
    "- The automated approach does not replace field validation and traditional Indigenous knowledge.\n",
    "- Possible biases in historical data and consulted literature.\n",
    "\n",
    "> **Next Steps:**\n",
    "- Integrate ethnographic data and crowdsourced local reports.\n",
    "- Explore multimodal AI models for analysis of historical images and texts.\n",
    "- Expand the analysis to other regions of the Amazon and compare patterns.\n",
    "\n",
    "---\n",
    "## Further Reading and Resources\n",
    "- [Archaeology in the Amazon: Recent Discoveries](https://www.nature.com/articles/d41586-018-07287-2)\n",
    "- [MapBiomas Project](https://mapbiomas.org/)\n",
    "- [GEDI NASA Mission](https://gedi.umd.edu/)\n",
    "- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [Folium Interactive Maps](https://python-visualization.github.io/folium/)\n",
    "- [Indigenous Knowledge and Remote Sensing](https://www.sciencedirect.com/science/article/pii/S2351989422000132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowing issues \n",
    "\n",
    "Sometimes Google Earth Engine returns a 500 error. If this happens, simply try running the cell again."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
